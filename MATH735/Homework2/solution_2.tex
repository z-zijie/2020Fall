\documentclass{article}
% --- Modify margins --- %
\usepackage{geometry}
\geometry{a4paper,scale=0.8}
% --- Involved packages --- %
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{physics}
\usepackage{enumitem}
\usepackage{listings}
\usepackage{xcolor}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{appendix}

\renewcommand{\familydefault}{\sfdefault}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
	% backgroundcolor=\color{backcolour},
	commentstyle=\color{codegreen},
	keywordstyle=\color{magenta},
	numberstyle=\tiny\color{codegray},
	stringstyle=\color{codepurple},
	basicstyle=\sffamily\footnotesize,
	breakatwhitespace=false,
	breaklines=true,
	captionpos=b,
	keepspaces=true,
	% numbers=left,
	numbersep=5pt,
	showspaces=false,
	showstringspaces=false,
	showtabs=false,
	tabsize=2
}

\lstset{style=mystyle}

% --- NewCommand --- %
% \newcommand{\norm}[1]{\left\lVert#1\right\rVert}

\usepackage{hyperref}
\hypersetup{
	colorlinks=true,
	linkcolor=blue,
	filecolor=magenta,
	urlcolor=cyan,
}

\urlstyle{same}






% --- Title information --- %
\title{MATH 735 - Fall 2020\\
		{\Large \textbf{Homework 2}}\\
		{\normalsize \textbf{Due : 11/04, 2020}}
	}
\author{Zijie Zhang}
\date{\today}



% --- main --- %
\begin{document}
	\maketitle

	% Problem 1
	\section*{Problem 1}
	$X,Y$ are two independent Brownian motions, compute $[X,Y]$.
	\begin{proof}
		By (2.13) in \textit{Timoâ€™s notes}.
		$$[X,Y]_t = \lim_{|\pi|\to 0}\sum_i(X_{t_{i+1}} - X_{t_{i}})(Y_{t_{i+1}}-Y_{t_{i}})$$
		We need prove $\mathbb{E}\left[\sum_{i}(X_{t_{i+1}} - X_{t_{i}})(Y_{t_{i+1}}-Y_{t_{i}})\right] \to 0$
		which is 
		$$\sum_i \mathbb{E}\left[X_{t_{i+1}}Y_{t_{i+1}}\right] 
		+ \sum_i \mathbb{E}\left[X_{t_{i}}Y_{t_{i}}\right]
		- \sum_i \mathbb{E}\left[X_{t_{i}}Y_{t_{i+1}}\right]
		- \sum_i \mathbb{E}\left[X_{t_{i+1}}Y_{t_{i}}\right] \to 0$$
		By the independence of $X$ and $Y$, all the expectations above are 0.
		So $$[X, Y] = 0 \text{ if } X ,Y \text{ are two independent Brownian motions}$$
	\end{proof}

	% Problem 2
	\section*{Problem 2}
	Compute the quadratic variations [N] and [M] where N is Poisson process and M is compensated Poisson process.
	\begin{enumerate}
		\item \begin{align*}
			[N]_t &= \sum_{0 \leqslant s \leqslant t}\left(\Delta N_s\right)^2 = N_t\\
			[N] &= N
		\end{align*}
		\item $$M = N - \lambda t$$
		By Lemma A.10 and Lemma A.11, we know that $[f](T) = 0$ if $f$ is continuous. So we have
		$$(\Delta(N_s - \lambda s))^2 = (\Delta N_s)^2$$
		Thus, $$[M] = N$$
	\end{enumerate}

	% Problem 3
\section*{Problem 3}
Suppose $M$ is a right-continuous square-integrable martingale with stationary independent increments: for all $s, t \geqslant 0, M_{s+t}-M_s$ is independent of $\mathcal{F}_s$ and has the same distribution as $M_t-M_0$. Then $\langle M \rangle_t = t \cdot E[M_1^2 - M_0^2] $
\begin{proof}
	The deterministic, continuous function $t \to t \cdot E[M_1^2 - M_0^2]$ is predictable. For any $t>0$ and integer $k$
	$$E[M_{kt}^2 - M_0^2] = \sum_{j=0}^{k-1}E[M_{(j+1)t}^2 - M_{jt}^2] = \sum_{j=0}^{k-1}E[(M_{(j+1)t} - M_{jt})^2] = kE[(M_t - M_0)^2] = kE[M_t^2 - M_0^2]$$
	Using this twice, for any rational $k/n$,
	$$E[M_{k/n}^2 - M_0^2] = kE[M_{1/n}^2 - M_0^2] = (k/n)E[M_1^2 - M_0^2]$$
	Given an irrational $t>0$, pick rationals $q_n \to t$. Fix $T\geqslant q_m$. By right-continuity of paths, $M_{q_m} \to M_{t}$ almost surely. Uniformly integrability of $\{M_{q_m}^2\}$ follows by the submartingale property
	$$0\leqslant M_{q_m}^2 \leqslant E[M_{T}^2|\mathcal{F}_{q_m}]$$
	and Lemma B.16. Uniformly integrability gives convergence of expectations $E[M_{q_m}^2] \to E[M_{t}^2]$. Applying this above gives $$E[M_t^2 - M_0] = tE[M_1^2-M_0^2]$$
	Now we can check the martingale property.\begin{align*}
		E[M_t^2|\mathcal{F}_{s}] &= M_s^2+E[M_t^2-M_s^2|\mathcal{F}_{s}]\\
		& = M_s^2 + E[(M_t-M_s)^2|\mathcal{F}_s]\\
		& = M_s^2 + E[(M_{t-s} - M_0)^2]\\
		& = M_s^2 + E[M_{t-s}^2 - M_0^2]\\
		& = M_s^2 + (t-s)E[M_1^2-M_0^2]
	\end{align*}
\end{proof}

% Problem 4
\section*{Problem 4}
	\begin{align*}
		C_n \leqslant \sum_{i=1}^n (B_{t_{i}}-B_{t_{i-1}})^2 h(B_{t_{i-1}}, B_{t_{i}})
	\end{align*}
	We need prove $(B_{t_{i}}-B_{t_{i-1}})^2$ converges to 0.
	\begin{align*}
		E[(B_{t_{i}}-B_{t_{i-1}})^2] &= \Delta t \to 0\\
		E[(B_{t_{i}}-B_{t_{i-1}})^4] &= 3(\Delta t)^2 \to 0
	\end{align*}
	So, $(B_{t_{i}}-B_{t_{i-1}})^2$ converges to 0. Thus $C_n \to 0$.

% Problem 5
\section*{Exercise 3.1}
	By the definition of It\^{o} integral. Consider a partation $\pi: 0 = t_0 < t_1 < \cdots < t_n = t$.
	$$tB_t = \sum_{j} s_j \Delta B_j + \sum_{j} B_j \Delta s_j$$
	$$\lim_{|\pi|\to 0} \sum_{j} s_j \Delta B_j = \int_0^t sdB_s$$
	$$\lim_{|\pi|\to 0} \sum_{j} B_j \Delta s_j = \int_0^t B_sds$$
	From the definition,
	$$\int_0^t sdB_s = tB_t - \int_0^t B_sds$$

\section*{Exercise 3.3}
	\begin{enumerate}
		\item \begin{align*}
			\mathbb{E}\left[X_t|\mathcal{H}_s\right]
			& = \mathbb{E}\left[\mathbb{E}[X_t|\mathcal{N}_s]|\mathcal{H}_s\right]\\
			& = \mathbb{E}\left[X_s|\mathcal{H}_s\right]\\
			& = X_s
		\end{align*}
		\item $$\mathbb{E}[X_t] = \mathbb{E}\left[\mathbb{E}[X_t|\mathcal{H}_0]\right] = \mathbb{E}[X_0]$$
		\item The probability of winning or losing in gambling is 1/2. If win $X_t = t$, if loss $X_t = -t$. Consider the expectation,
		$$\mathbb{E}[X_t] = 0$$
		$$\mathbb{E}[X_t|\mathcal{H}_s] = t , X_s \geqslant 0$$
		$$\mathbb{E}[X_t|\mathcal{H}_s] = -t , X_s < 0$$
		Thus, $X_t$ is not a martingale.
	\end{enumerate}

\section*{Exercise 3.4}
	\begin{enumerate}
		\item No. Because the expectation $$\mathbb{E}[X_t|\mathcal{H}_s] = X_s + 4(t-s)$$
		\item No. Because the expectation \begin{align*}
			\mathbb{E}[X_t|\mathcal{H}_s]
			& = \mathbb{E}[B_t^2|\mathcal{H}_s]\\
			& = \mathbb{E}\left[B_s^2 + B_t^2 - B_s^2|\mathcal{H}_s\right]\\
			& = B_s^2 + \mathbb{E}[B_t^2 - B_s^2|\mathcal{H}_s]\\
			& = B_s^2 + \mathbb{E}\left[2B_s(B_t - B_s) + (B_t-B_s)^2|\mathcal{H}_s\right]\\
			& = B_s^2 + 2B_s\mathbb{E}\left[B_t - B_s|\mathcal{H}_s\right] + \mathbb{E}\left[(B_t-B_s)^2|\mathcal{H}_s\right]\\
			& = X_t + (t-s)
		\end{align*}
		\item Yes. \begin{align*}
			\mathbb{E}[X_t|\mathcal{H}_s]
			& = \mathbb{E}\left[t^2B_t - 2\int_0^t sB_sds|\mathcal{H}_s\right]\\
			& = \mathbb{E}\left[(t^2-s^2)B_s + t^2(B_t-B_s) - 2\int_s^t uB_udu|\mathcal{H}_s\right] + X_s\\
			& = X_s + (t^2-s^2)B_s - 2\int_s^t u\left(B_s+\mathbb{E}\left[B_u-B_s|\mathcal{H}_s\right]\right)du\\
			& = X_s + (t^2-s^2)B_s - 2B_s\int_s^t udu\\
			& = X_s
		\end{align*}
		\item Yes. \begin{align*}
			\mathbb{E}[X_t|\mathcal{H}_s]
			& = \mathbb{E}[B_1(t)B_2(t)|\mathcal{H}_s]\\
			& = \mathbb{E}[\left(B_1(s)+(B_1(t)-B_1(s))(B_2(s)+(B_2(t)-B_2(s)))\right)|\mathcal{H}_s]\\
			& = B_1(s)B_2(s) 
				+ B_1(s)\mathbb{E}[B_2(t)-B_2(s)|\mathcal{H}_s]
				+ B_2(s)\mathbb{E}[B_1(t)-B_1(s)|\mathcal{H}_s]
				+ \mathbb{E}[B_1(t)-B_1(s)|\mathcal{H}_s]\mathbb{E}[B_2(t)-B_2(s)|\mathcal{H}_s]\\
			& = B_1(s)B_2(s)\\
			& = X_s
		\end{align*}
	\end{enumerate}

\section*{Exercise 3.7}
	\begin{proof}
	\begin{enumerate}
		\item When n = 1
			$$I_1(t) = \int_0^t I_0(s)dB_s = B_t = t^{\frac{1}{2}}h_1\left(\frac{B_t}{\sqrt{t}}\right)$$
		\item When n = 2
			$$I_2(t) = 2\int_0^t I_1(s)dB_s = 2\int_0^t B_s dB_s = B_t^2 - t = t^{\frac{2}{2}}h_2\left(\frac{B_t}{\sqrt{t}}\right)$$
		\item When n = 3
			$$I_3(t) = 3\int_0^t I_2(s)dB_s = 3\int_0^t (B_s^2 - s)dB_s = B_t^3 - 3tB_t = t^{\frac{3}{2}}h_3\left(\frac{B_t}{\sqrt{t}}\right)$$
		\item For all n, let
		$$H_{n}(x) = \frac{t^{\frac{n}{2}}}{n!}h_n\left(\frac{x}{\sqrt{t}}\right)$$
		\begin{align*}
			H_{n+1}(x) &= \frac{t^{\frac{n+1}{2}}}{(n+1)!}h_{n+1}\left(\frac{x}{\sqrt{t}}\right)\\
			&=\frac{t^{\frac{n+1}{2}}}{(n+1)!}\left[\frac{x}{\sqrt{t}}h_n\left(\frac{x}{\sqrt{t}}\right) - nh_{n-1}\left(\frac{x}{\sqrt{t}}\right)\right]\\
			&=\frac{x}{n+1}H_n(x) - \frac{t}{n+1}H_{n-1}(x) 
		\end{align*}
		By the properties of Hermite polynomials,
		$$H_n'(x) = H_{n-1}(x) = -\frac{1}{2}H_{n-2}(x)$$
		Then we have $$dH_{n+1}(B_t) = H_n(B_t)dB_t$$
		That means$$H_{n+1}(B_t) = \int_0^t H_n(B_t)dB_t$$
		$$\frac{t^{\frac{n+1}{2}}}{(n+1)!}h_{n+1}\left(\frac{B_t}{\sqrt{t}}\right) = \int_0^t \frac{t^{\frac{n}{2}}}{n!}h_{n}\left(\frac{B_t}{\sqrt{t}}\right)dB_t$$
	\end{enumerate}
	\end{proof}

\section*{Exercise 4.1}
	\begin{enumerate}
		\item $g(t,x) = x^2$ then $dB_t^2 = 2B_tdB_t+dt$.\\
			$u = 1, v = 2B_t$.
		\item $g(t,x) = 2+t+e^x$ then $dX_t = dt + e^{B_t}dB_t + \frac{1}{2}e^{B_t}dt = (1+\frac{1}{2}e^{B_t})dt + e^{B_t}dB_t$.\\
			$u = 1+e^{B_t}, v = e^{B_t}$.
		\item $g(t,x_1,x_2) = x_1^2 + x_2^2$ then $dX_t = 2dt + 2B_1dB_1(t) + 2B_2dB_2(t)$.\\
			$u = 2, v = \begin{bmatrix}
				2B_1 \\
				2B_2
			\end{bmatrix}$.
		\item $dX_t = (dt, dB_t)$.
		\item $dX_t = (dB_1(t)+dB_2(t)+dB_3(t), 2B_2(t)dB_t(2)+2dt-B_1(t)dB_3(t)-B_3(t)dB_1(t))$.
	\end{enumerate}

\section*{Exercise 4.11}
	\begin{enumerate}
		\item \begin{align*}
			dX_t
			& = \frac{1}{2}e^{\frac{1}{2}t}\cos{(B_t)}dt - e^{\frac{1}{2}t}\sin{(B_t)}dB_t - \frac{1}{2}e^{\frac{1}{2}t}\cos{(B_t)}dt\\
			& = -e^{\frac{1}{2}t}\sin{(B_t)}dB_t
		\end{align*}
		So, this is martingale.
		\item $$dX_t = e^{\frac{1}{2}t}\cos{(B_t)}$$
		So, this is martingale.
		\item Let $f(x,t) = (x+t)e^{-x-\frac{1}{2}t}$
		\begin{align*}
			\frac{\partial f}{\partial t}
			& = e^{-x-\frac{1}{2}t} - \frac{1}{2}(x+t)e^{-x-\frac{1}{2}t}\\
			& = \left(1-\frac{1}{2}x - \frac{1}{2}t\right)e^{-x-\frac{1}{2}t}\\
			\frac{\partial f}{\partial x}
			& = \left(1-x-t\right)e^{-x-\frac{1}{2}t}\\
			\frac{\partial^2 f}{\partial x^2}
			& = \left(x+t-2\right)e^{-x-\frac{1}{2}t}
		\end{align*}
		Thus, \begin{align*}
			dX_t
			& = \left(1-B_t-t\right)e^{-B_t\frac{1}{2}t}dB_t + 0\\
			& = \left(1-B_t - t\right)e^{-B_t-\frac{1}{2}t}dB_t
		\end{align*}
		So, this is martingale.
	\end{enumerate}
\end{document}
