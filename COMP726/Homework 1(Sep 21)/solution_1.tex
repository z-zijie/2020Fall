\documentclass{article}
% --- Modify margins --- %
\usepackage{geometry}
\geometry{a4paper,scale=0.8}
% --- Involved packages --- %
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsmath}

% --- Title information --- %
\title{CS 726 - Fall 2020\\
        {\Large \textbf{Homework \#1}}\\
        {\normalsize \textbf{Due : 09/21/2020, 5pm}}
    }
\author{Zijie Zhang}
\date{\today}


% --- main --- %

\begin{document}
    \maketitle

% --- Question 1 --- %
\section*{Question 1}
    \begin{proof}
        When $\boldsymbol{x} = (1,0,0,\cdots,0)$,
        $$ ||x||_q = \left(\sum_{i=1}^n|x_i|^q\right)^{\frac{1}{q}} = 1 $$
        $$ ||x||_p = \left(\sum_{i=1}^n|x_i|^p\right)^{\frac{1}{p}} = 1 $$
        $$ ||x||_q = ||x||_p $$
        When $\boldsymbol{x} = (1,1,1,\cdots,1)$,
        $$ ||x||_p = \left(\sum_{i=1}^n|x_i|^p\right)^{\frac{1}{p}} = d^{\frac{1}{p}} $$
        $$ d^{\frac{1}{p}-\frac{1}{q}}||x||_q = d^{\frac{1}{p}-\frac{1}{q}} \left(\sum_{i=1}^n|x_i|^q\right)^{\frac{1}{q}} = d^{\frac{1}{p}} $$
        $$ ||x||_p = d^{\frac{1}{p}-\frac{1}{q}}||x||_q $$
    \end{proof}

% --- Question 2 --- %
\section*{Question 2}
    \begin{proof}
        By \textbf{Lemma 2.3} in Lecture notes:\\
        $f$ is smooth w.r.t. $||\cdot||_2$ iff.
        $$-L\cdot \mathcal{I} \leqslant \nabla^2 f(x) \leqslant L \cdot \mathcal{I}$$
        So $L \geqslant ||\nabla^2 f(x)||_2$, the smallest constant $L_2$ is $||\nabla^2 f(x)||_2$.\\


    \end{proof}

% --- Question 3 --- %
\section*{Question 3}
    \begin{proof}
        If $\alpha_1$ and $\alpha_2$ are two aribitray nonnegative real numbers such that $\alpha_1+\alpha_2=1$ then
        the convexity of $f$ implies
        $$ \forall x_1, x_2, \ f(\alpha_1 x_1 + \alpha_2 x_2) \leqslant \alpha_1 f(x_1) + \alpha_2 f(x_2) $$
        This can be easily generalized: if $\alpha_1, \cdots, \alpha_n$ are nonnegative real numbers such that
        $\alpha_1 + \cdots + \alpha_n = 1$, then
        $$f(\alpha_1 x_1 + \cdots + \alpha_n x_n) \leqslant \alpha_1 f(x_1) + \cdots \alpha_n f(x_n)$$
        for any $x_1, \cdots, x_n$.\\
        By induction:\\
        The statement is true for $n=2$. Suppose it is also for some $n$, one needs to prove it for $n+1$.
        At least one of the $\alpha_i$ is strictly positive, say $\alpha_1$: therefore by convexity inequality:
        \begin{align*}
            f\left(\sum_{i=1}^{n+1}\alpha_i x_i\right) &= f\left(\alpha_1 x_1 + (1-\alpha_1)\sum_{i=2}^{n+1}\frac{\alpha_i}{1-\alpha_1}x_i\right)\\
            &\leqslant \alpha_1 f(x_1) + (1-\alpha_1) f\left(\alpha_1 x_1 + (1-\alpha_1)\sum_{i=2}^{n+1}\frac{\alpha_i}{1-\alpha_1}x_i\right)
        \end{align*}
        Since $$ \sum_{i=2}^{n+1}\frac{\alpha_i}{1-\alpha_1} = 1 $$
        one can apply the induction hypotheses to the last term in the previous formula to obtain the result, namely the finite form of the Jensen's inequality.
        The finite form can be rewritten as:
        $$f\left(\sum_{i=1}^k \alpha_i x_i\right) \leqslant \sum_{i=1}^k \alpha_i f(x_i)$$
    \end{proof}

% --- Question 4 --- %
\section*{Question 4}
    \begin{proof}
        \indent
        \begin{itemize}
            \item[(i)]
            
            \item[{(ii)}]
            By the convexity of function $f$, for all $y \in \mathbb{R}^d$, $x \in \mathbb{R}^d$, $|f(x)| = M$. For $\alpha \in [0,1]$
            $$M \geqslant \alpha f(x) + (1-\alpha)f(y) \geqslant f\left(\alpha x + (1-\alpha) y\right)\geqslant f(y)$$
            $$f(x)\geqslant f(y)$$
            Thus, $f$ has maximum, this contradicts that $f$ is a convex function.
        \end{itemize}

    \end{proof}

% --- Question 5 --- %
\section*{Question 5}
    \begin{proof}
        This can be verified by using the definition of convex function and convex set.
        \begin{itemize}
            \item $\Rightarrow$
                Suppose $(x,a_1),\ (y,a_2) \in epi(f)$ then $f(x)\leqslant a_1,\ f(y)\leqslant a_2$ \\
                For any $\lambda \in [0,1]$, by convexity of $f$,
                $$f(\lambda x + (1-\lambda)y) \leqslant \lambda f(x) + (1-\lambda) f(y) \leqslant \lambda a_1 + (1-\lambda) a_2$$
                This implies that $$\lambda (x, a_1) + (1-\lambda) (y, a_2) \in epi(f)$$
                Hence $epi(f)$ is convex.
            \item $\Leftarrow$
                Let $x,\ y \in \mathbb{R}^d$, since $(x, f(x))$ and $(y, f(y))$ lie in $epi(f)$
                by convexity of epigraph set, we have
                $\forall \lambda \in [0,1]$
                $$(\lambda x+(1-\lambda)y,\ \lambda f(x) + (1-\lambda)f(y)) \in epi(f)$$
                By definition, $f(\lambda x+(1-\lambda)y) \leqslant \lambda f(x) + (1-\lambda)f(y))$.\\
                Hence, function $f$ is convex.
        \end{itemize}
    \end{proof}

% --- Question 6 --- %
\section*{Question 6}
    \begin{proof}
        By \textbf{Theorem 2.1}, we know
        $$f(y) = f(x) + \int_0^1 \langle\nabla f\left(x+t(y-x)\right), y-x\rangle dt$$
        thus, $f(y)-f(x)-\langle\nabla f(x), y-x\rangle = \int_0^1 \langle\nabla f\left(x+t(y-x)\right)-\nabla f(x), y-x\rangle dt$\\
        By the convexity of $f$, the integral is nonnegative. So, we proved
        $$f(y) \geqslant f(x) + \langle f(x), y-x\rangle$$
    \end{proof}

% --- Question 7 --- %
\section*{Question 7}
    \begin{proof}
        A is symmetric therefore diagonalisable in an orthonormal basis of $\mathbb{R}^d$. Let $(e_i)$ be that basis and write
        $$x = \sum_i x_i\cdot e_i,\ Ae_i = \lambda_i e_i$$
        $$f(x)=\frac{\sum_i \lambda_i x_i^2}{\sum_i x_i^2}$$
        Then, \begin{align*}
            x^T A x &= x^T A \sum_i x_i e_i\\
            &= x^T \sum_i x_i \lambda_i e_i\\
            &= \sum_i \lambda_i x_i^2
        \end{align*}
        Thus, $$\lambda_1 \leqslant f(x) \leqslant \lambda_d$$
        So we proved, \begin{itemize}
            \item[(i)] $$x^T A x \geqslant \lambda_1 ||x||_2$$
            \item[(ii)] $$x^T A x \leqslant \lambda_d ||x||_2$$
        \end{itemize}
    \end{proof}

% --- Question 8 --- %
\section*{Question 8}
    \begin{proof}
        $A$ is positive semidefinite means $x^T A x \geqslant 0$ for all $x\in\mathbb{R}^d$.\\
        In \textbf{Question 7} we have prove the symmetric matrix $A$ satisfies $x^T A x \geqslant \lambda_1 ||x||_2$ for all $x\in\mathbb{R}^d$.\\
        So, we just need the smallest eigenvalue of matrix $A$ is nonnegative.\\
        By \textbf{LDL} decomposition, we know
        $$L^{-1} = \begin{bmatrix}
            1 & 0\\
            1/2 & 1 & 0\\
            1/3 & 2/3 & 1 & 0\\
            \vdots & & & & \ddots\\
            1/d-1 & 2/d-1 & 3/d-1 & \cdots & d-2/d-1 & 1 & 0\\
            1 & 1 & 1 & 1 & 1 & 1 & 1\\
        \end{bmatrix}$$
        Then $$D = L^{-1} A {L^{-1}}^{T}$$
        Obviously, the eigenvalues of $A$ are all nonnegative.\\
        By the way, $\det(A) = 0$, 0 is the smallest eigenvalue of matrix $A$.
    \end{proof}
\end{document}